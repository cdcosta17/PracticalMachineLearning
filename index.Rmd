---
title: "Practical Machine Learning Assignment"
author: "Clifford D'costa"
date: "5/28/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prediction Assignment Writeup

## Summary
The goal of this project is to predict the manner in which users did a set of exercises.This report describes how the model was built, how cross validation was used, what the expected out of sample error is. Using the prediction model built, we shall also attempt to predict 20 different test cases.

## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://groupware.les.inf.puc-rio.br/har). (see the section on the Weight Lifting Exercise Dataset).

## Data
The training data for this project are available here:
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>

The test data are available here:
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

The data for this project come from this [source](http://groupware.les.inf.puc-rio.br/har.).

## Data Loading and Preparation:
We first begin by setting the working directory and loading the relevant libraries
```{r}
library(knitr)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
setwd("~/Documents/R /PracMacLearning")
```

We load the data from the source and store them in training and testing variables
```{r}
#Loading the data
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(trainUrl, "pml-training.csv")
training <- read.csv("pml-training.csv")

testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(testUrl, "pml-testing.csv")
testing <- read.csv("pml-testing.csv")
dim(training)
str(training,list.len=12)
```

There are over 19,000 rows and 160 columns in the dataset, but we can see that many of these columns have missing values. Also the first 6 columns contain information related to users(such as names,timestamps). We can remove some of these unnecessary columns to clean up the data

```{r}
# Removing the first 6 columns as the have user info
training<-training[,7:160]
testing<-testing[,7:160]

# Removing columns that have over 90% NA values
NACol <- sapply(training,function(x) mean(is.na(x))) >0.90
training <- training[,NACol==FALSE]
testing <- testing[,NACol==FALSE]

# Removing columns that have Near Zero Variance(NZV)
nzv <- nearZeroVar(training)
training <- training[,-nzv]
testing <- testing[,-nzv]

dim(training)
```

The data is now cleaned of any unecessary columns, There are 53 columns(54 including classe), that can be used to fit the models. Before we start fitting models, we shall split the training data into two sets, so that we can use one for training and one for testing. The original testing data will be used only for validation. 
```{r}
# Splitting the training dataset into two
inTrain <- createDataPartition(training$classe,p=.7,list = FALSE)
trainData <- training[inTrain,] ; testData <- training[-inTrain,]
```

## Building Prediction Models
Since we split the training data into trainData(70%) and testData(30%), we can use cross-validation to find the best models. First we build a model on trainData and then we use the model built to predict the values of the testData, we can then check the accuracy of the model by building a confusion matrix. 

### Random Forest
```{r}
# Random Forest Model
set.seed(2017)
controlRF <- trainControl(method = "cv",number=3,verboseIter = FALSE)
modFitRF <- train(classe~.,data=trainData,method="rf",trControl=controlRF)
predictRF <- predict(modFitRF,newdata=testData)
confRF <- confusionMatrix(predictRF,testData$classe)
confRF
```
This model looks promising with a 99.86% accuracy rate, however since we modelled it with all 53 remaining classifiers the model took some time to run. The model took approximately 6 minutes to run.

### Generalised Boosting Model
```{r}
set.seed(2017)
controlGBM <- trainControl(method="repeatedcv",number=5,repeats=1)
modFitGBM <- train(classe~.,data=trainData,method="gbm", trControl=controlGBM,verbose=FALSE)
predictGBM <- predict(modFitGBM,newdata=testData)
confGBM <- confusionMatrix(predictGBM,testData$classe)
confGBM
```
This model has an accuracy rate of 98.81% and it ran almost twice as fast as the random forest model(took 3 mins to run). It has a slightly lower accuracy but is quicker.

###Decision Tree
Finally we take a look at a decision tree model
```{r}
modFitTree <- rpart(classe~.,data = trainData,method = "class")
prp(modFitTree)
```

We can use the decision tree to identify what are the most important classifiers in the model. From the tree we can see that the top classifiers are roll_belt, pitch_forearm,magnet_dumbbell_y,total_accel_dumbbell,roll_forearm,magnet_dumbbell_y,magnet_dumbbell_z,accel_forearm_x and num_window. We can probably use these classifiers to make the random forest model more efficient

### Final Model(Random Forest)
```{r}
# Choosing the Random forest model
modFit <- train(classe~roll_belt+pitch_forearm+magnet_dumbbell_y+total_accel_dumbbell+roll_forearm+magnet_dumbbell_y+magnet_dumbbell_z
                +accel_forearm_x+num_window,data=trainData,method="rf",trControl=controlRF)
predict <- predict(modFit,newdata=testData)
confMat <- confusionMatrix(predict,testData$classe)
confMat
```
This model still maintains a 99.8% accuracy even with fewer classifiers. Since there were fewer classifiers the model was much faster than the original random forest model and completed within 2 minutes. 

## Results
From all the models that we ran we can see that random forests were the most accurate with a 99.8% accuracy followed by generalised boosting with 98.81% accuracy. The best model for this data is the final random forest model that we ran, which was the quickest and had high accuracy. Since we did not train the data on the testData it gives us an accurate Out of Sample Error rate, which we can calculate as: 100%-Accuracy = 100-99.8 = 0.2%

## Predicting the Test Data
The original testing data has not been touched so far, so we can be sure that the model is not over-fitted to the data, we can apply the random forest model to identify the classe for the testing data.
```{r}
predictions <- predict(modFit,newdata=testing)
predictions
```


